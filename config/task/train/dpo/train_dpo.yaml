name: "dpo_v1"
args: 
  beta: 0.1
  # Optimizing
  rpo_alpha: 1.0
  optim: "adamw_8bit"
  adam_beta1: 0.9
  adam_beta2: 0.995
  learning_rate: 0.00005
  lr_scheduler_type: "cosine"
  # Epochs and effective batch size of 8 (4 gpus)
  num_train_epochs: 3
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 1
  per_device_eval_batch_size: 4 
  # Efficient training
  gradient_checkpointing: True 
  gradient_checkpointing_kwargs:
    use_reentrant: False
  dataloader_pin_memory: True
  dataloader_num_workers: 4
  # Saving etc. 
  overwrite_output_dir: True 
  load_best_model_at_end: True

lora:
  r: 16
  lora_alpha: 32 
  use_rslora: True 

sampling:
  train_frac: 0.8
  head_frac: 0.30
